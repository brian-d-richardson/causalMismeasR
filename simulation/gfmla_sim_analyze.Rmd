---
title: "MCCS G-Formula Simulation Results"
author: "Brian Richardson"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  html_document:
    code_folding: hide
---

```{r message = F, warning = F}

rm(list = ls())
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggh4x)
library(kableExtra)
library(devtools)
load_all()

```

The goal of this program is to evaluate the Monte Carlo corrected score G-formula (MCCS G-FMLA) estimator in the presence of confounding and measurement error.

## Setup

Consider the following setup:

* two independent binary confounders $L_1 \sim \textrm{Bernoulli}(0.5), L_2 \sim \textrm{Bernoulli}(0.2)$,
* univariate continuous exposure $A$ with $A|L_1,L_2 \sim N(2+0.3L_1-0.5L_2, 0.6)$,
* mismeasured exposure $A^* = A + \epsilon$, where $\epsilon \sim N(0, 0.25)$,
* binary outcome $Y$ with $\textrm{E}(Y|A,L) = \textrm{logit}^{-1}(-2+0.7A-0.6L_1+0.4L_2-0.4AL_1-0.2AL_2)$.

This data generating process leads to a mean potential outcome at $a$ of $\textrm{E}\{Y(a)\} = 0.4\textrm{logit}^{-1}(-2+0.7a) + 0.4\textrm{logit}^{-1}(-2.6+0.3a) + 0.1\textrm{logit}^{-1}(-1.6+0.5a) + 0.1\textrm{logit}^{-1}(-2.2+0.1a)$.

The estimand of interest in these simulations is the dose response curve $\{\textrm{E}[Y(a)] : a \in \mathcal{A}\}$.

## Methods

In these simulations, we compare six methods:

1) Oracle GLM
2) Oracle G-Formula
3) Naive GLM
4) Naive G-Formula
5) Corrected GLM
6) Corrected G-Formula

The GLM methods fit a logistic regression mode of $Y$ on $A$ and $L$ with all interaction terms between $A$ and $L$, then estimate $\textrm{E}\{Y(a)\}$ with $\textrm{logit}^{-1}(\widehat\beta_0+\widehat\beta_aa)$. The G-formula methods fit the same logistic regression models, then estimate $\textrm{E}\{Y(a)\}$ with $n^{-1}\sum_{i=1}^n\textrm{logit}^{-1}(\widehat\beta_0+\widehat\beta_Aa+\widehat\beta_{L_1}L_{1i}+\widehat\beta_{L_2}L_{2i}+\widehat\beta_{A:L_1}aL_{1i}+\widehat\beta_{A:L_2}aL_{2i})$.

The oracle methods use the true exposure values $A$, which are not observed in practice. The naive methods treat $A^*$ as $A$, assuming there is no measurement error. The corrected methods use the Monte-Carlo corrected score versions of the estimating functions, which account for measurement error.

## Results

We first check for simulations with errors.

```{r}

# load simulation results from each of 10 clusters
sim.out.list <- lapply(
  X = 0:9,
  FUN = function(clust) {
    cbind(clust,
          read.csv(paste0("sim_data/gfmla_data/sd",
                          clust, ".csv")))
  })


# combine simulation results into 1 data frame
sim.out <- bind_rows(sim.out.list)

# true estimands
g <- c(-2, 0.7, -0.6, 0.4, -0.4, -0.2)          # outcome model parameters
a <- 0:4                                       # exposure value of interest
EYa.true <- 0.4 * inv.logit(-2 + 0.7 * a) +    # true dose response curve at a
  0.4 * inv.logit(-2.6 + 0.3 * a) +
  0.1 * inv.logit(-1.6 + 0.5 * a) +
  0.1 * inv.logit(-2.2 + 0.1 * a)

# number of sims per setting
n.rep <- nrow(sim.out) / n_distinct(dplyr::select(sim.out, n, B, vare))

# make long data frame
sim.out.long <- sim.out %>% 
  pivot_longer(cols = starts_with("est") | starts_with("ste"), 
               names_to = "method.aa",
               values_to = "val") %>% 
    mutate(method = factor(substr(method.aa, 5, 6),
                           levels = c("OL", "OG",
                                      "NL", "NG",
                                      "CL", "CG")),
           aa = factor(substr(method.aa, 8, 8)),
           a = factor(a[aa]),
           name = factor(substr(method.aa, 1, 3)),
           EYa.true = EYa.true[aa]) %>% 
  dplyr::select(-method.aa) %>% 
  group_by(clust, n, B, vare, method, aa, a, name) %>% 
  mutate(id = row_number()) %>% 
  pivot_wider(names_from = name,
              values_from = val,
              id_cols = c(clust, n, B, vare, method, aa, a, EYa.true, id))

```

We first check for simulations with errors.

```{r}

# summarize proportion of missing data by setting
sim.out.long %>% 
  filter(aa == 1) %>% 
  group_by(method, n, B, vare) %>% 
  summarise(prop.error = mean(is.na(est))) %>% 
  filter(prop.error > 0) %>% 
  ungroup()

```

```{r}

# extract simulation parameters
n <- unique(sim.out$n)
B <- unique(sim.out$B)
vare <- unique(sim.out$vare)

# make labels for plots
method.labs <- c("Oracle GLM",
                 "Oracle G-Formula",
                 "Naive GLM",
                 "Naive G-Formula",
                 "Corrected GLM",
                 "Corrected G-Formula")
names(method.labs) <- c("OL", "OG",
                        "NL", "NG",
                        "CL", "CG")

n.labs <- paste0("n = ", n)
names(n.labs) <- n

B.labs <- paste0("B = ", B)
names(B.labs) <- B

a.labs <- paste0("a = ", a)
names(a.labs) <- a

vare.labs <- paste0("sigma_e = ", vare)
names(vare.labs) <- vare

# colorblind friendly pallette
pal_light <- c('#EE6677', '#228833', '#4477AA', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB')
pal_dark <- c('#991122', '#114419', '#223b55', '#6b611d', '#117799', '#55193b', '#5d5d5d')

```

```{r}

# separate plots for each sample size
plot.by.a <- function(a.,
                      method. = names(method.labs),
                      n. = n,
                      vare. = vare,
                      est_cutoff = Inf) {
  
  # data for plot
  plot.dat <- sim.out.long %>% 
    filter(a == a.,
           method %in% method.,
           n %in% n.,
           vare %in% vare.) %>%  
    mutate(remove = abs(est - EYa.true) > est_cutoff,
           lab.y = 0.55*(est_cutoff - EYa.true))
  
  # count removed observations
  remove.dat <- plot.dat %>% 
    group_by(method, n, B, vare, lab.y, EYa.true) %>% 
    summarise(n.remove = sum(remove, na.rm = T)) %>% 
    mutate(label = ifelse(n.remove > 0,
                          paste0(n.remove, "\n",
                                 method.labs[method],
                                 "\npoints outside\nrange"), ""),
           lab.y = ifelse(n.remove > 0, lab.y, EYa.true))

  # create plot
  plot.dat %>% 
    filter(remove == F) %>% 
  ggplot(aes(x = method,
             y = est,
             fill = method,
             color = method)) +
    geom_boxplot() +
    geom_text(data = remove.dat,
              aes(x = 2, y = lab.y, label = label),
              size = 2, color = "black") +
    stat_summary(fun = mean,
                 geom = "point",
                 shape = 8,
                 size = 2,
                 orientation = "x",
                 show.legend = F) +
    geom_hline(aes(yintercept = EYa.true),
               linetype = "dashed",
               color = "orange") +
    facet_grid(vare ~ n,
               scales = "free",
               labeller = labeller(n = n.labs,
                                   vare = vare.labs)) +
    labs(y = "Parameter Estimate",
         fill = "Method",
         color = "Method") +
    ggtitle(paste0("Empirical Distribution of Parameter Estimates"),
            subtitle = paste0("a = ", a., "; ",
                              n.rep, " simulations per setting")) +
    theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.text.x = element_blank()) +
    scale_fill_manual(values = pal_light,
                      labels = method.labs) +
    scale_color_manual(values = pal_dark,
                      labels = method.labs)
}

```

### Plots of Simulation Results

Below are boxplots of the empirical distributions of estimators for $\textrm{E}\{Y(a)\}$ at exposure values $a = 0,1,2,3,4$.

```{r message = F, eval = F}

plot.by.a(a. = 0)
plot.by.a(a. = 1)
plot.by.a(a. = 2)
plot.by.a(a. = 3)
plot.by.a(a. = 4)

```

```{r message = F}

plot.by.a(a. = 0, est_cutoff = 4)
plot.by.a(a. = 1, est_cutoff = 3)
plot.by.a(a. = 2, est_cutoff = 2)
plot.by.a(a. = 3, est_cutoff = 2)
plot.by.a(a. = 4, est_cutoff = 2)

```

### Primary Plot

This plot shows only the simulation setting considered in the g-formula simulations in the original Blette submission, with $a=3$, $n=800$, $\sigma_e^2=0.25$, and only includes g-formula results.

```{r}

plot.by.a(a. = 3, method. = c("OG", "NG", "CG"), n. = 800, vare. = 0.25)

```

### Primary Table

Below is a table of the setting considered in the g-formula simulations in the original Blette submission, with $a=3$, $n=800$, $\sigma_e^2=0.25$. It includes empirical bias, empirical standard error (ESE), average estimated standard error (ASE), and mean squared error (MSE).

```{r}

tbl.a3 <- sim.out.long %>% 
  filter(a == 3,
         n == 800,
         vare == 0.25) %>% 
  group_by(method) %>% 
  summarise(bias = mean(est - EYa.true, na.rm = T),
            emp.se = sd(est, na.rm = T),
            est.se = mean(ste, na.rm = T),
            mse = mean((est - EYa.true)^2))

tbl.a3 %>% 
  kable(digits = 3,
        col.names = c("Method", "Bias", "ESE", "ASE", "MSE"),
        caption = "Estimated E{Y(a)} for a = 3, n = 800, vare = 0.25") %>%
  kable_styling("striped")

```

### Secondary Table

Below is a table with a sample size of $n=8000$.

```{r}

tbl.a3.n8k <- sim.out.long %>% 
  filter(a == 3,
         n == 8000,
         vare == 0.25) %>% 
  group_by(method) %>% 
  summarise(bias = mean(est - EYa.true, na.rm = T),
            emp.se = sd(est, na.rm = T),
            est.se = mean(ste, na.rm = T),
            mse = mean((est - EYa.true)^2))

tbl.a3.n8k %>% 
  kable(digits = 3,
        col.names = c("Method", "Bias", "ESE", "ASE", "MSE"),
        caption = "Estimated E{Y(a)} for a = 3, n = 8000, vare = 0.25") %>%
  kable_styling("striped")

```

### Complete Table

For completion, we also include a table with bias, ESE, and ASE across all settings considered.

```{r}

tbl <- sim.out.long %>% 
  group_by(a, method, n, B, vare) %>% 
  summarise(bias = mean(est - EYa.true, na.rm = T),
            emp.se = sd(est, na.rm = T),
            est.se = mean(ste, na.rm = T)) %>% 
  gather(key, value, bias:est.se) %>% 
  unite(Group, a, key) %>% 
  spread(Group, value)

setNames(tbl, sub(".+_", "", names(tbl))) %>% 
  kable(digits = 3) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 4,
                     "E{Y(0)}" = 3,
                     "E{Y(1)}" = 3,
                     "E{Y(2)}" = 3,
                     "E{Y(3)}" = 3,
                     "E{Y(4)}" = 3))
  
```

