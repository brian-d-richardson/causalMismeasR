---
title: "MCCS G-Formula Simulation Results"
author: "Brian Richardson"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  html_document:
    code_folding: hide
---

```{r message = F, warning = F}

rm(list = ls())
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggh4x)
library(kableExtra)
library(devtools)
load_all()

```

The goal of this program is to evaluate the Monte Carlo Corrected Score Inverse Probability Weighting (MCCS IPW) estimator in the presence of confounding and measurement error.

## Setup

Consider the following setup:

* two ndependent binary confounders $L_1 \sim \textrm{Bernoulli}(0.5), L_2 \sim \textrm{Bernoulli}(0.2)$,
* univariate continuous exposure $A$ with $A|L_1,L_2 \sim N(2+0.3L_1-0.5L_2, 0.6)$,
* mismeasured exposure $A^* = A + \epsilon$, where $\epsilon \sim N(0, 0.25)$,
* Binary outcome $Y$ with $\textrm{E}(Y|A,L) = \textrm{logit}^{-1}(-2+0.7A-0.6L_1+0.4L_2-0.4AL_1-0.2AL_2)$.

This data generating process leads to a mean potential outcome at $a$ of $\textrm{E}\{Y(a)\} = 0.4\textrm{logit}^{-1}(-2+0.7a) + 0.4\textrm{logit}^{-1}(-2.6+0.3a) + 0.1\textrm{logit}^{-1}(-1.6+0.5a) + 0.1\textrm{logit}^{-1}(-2.2+0.1a)$

## Methods

In these simulations, we compare six methods:

1) Oracle GLM
2) Oracle G-Formula
3) Naive GLM
4) Naive G-Formula
5) Corrected GLM
6) Corrected G-Formula

The GLM methods fit a logistic regression mode of $Y$ on $A$ and $L$ with all interaction terms between $A$ and $L$, then estimate $\textrm{E}\{Y(a)\}$ with $\textrm{logit}^{-1}(\widehat\beta_0+\widehat\beta_aa)$. The G-formula methods fit the same logistic regression models, then estimate $\textrm{E}\{Y(a)\}$ with $n^{-1}\sum_{i=1}^n\textrm{logit}^{-1}(\widehat\beta_0+\widehat\beta_Aa+\widehat\beta_{L_1}L_{1i}+\widehat\beta_{L_2}L_{2i}+\widehat\beta_{A:L_1}aL_{1i}+\widehat\beta_{A:L_2}aL_{2i})$.

The oracle methods use the true exposure values $A$, which are not observed in practice. The naive methods treat $A^*$ as $A$, assuming there is no measurement error. The corrected methods use the Monte-Carlo corrected score versions of the estimating functions, which (in theory) account for measurement error.

## Results

```{r}

# load simulation results from each of 10 clusters
sim.out.list <- lapply(
  X = 0:9,
  FUN = function(clust) {
    cbind(clust,
          read.csv(paste0("sim_data/gfmla_data/sd",
                          clust, ".csv")))
  })


# combine simulation results into 1 data frame
sim.out <- bind_rows(sim.out.list)

# true estimands
g <- c(-2, 0.7, -0.6, 0.4, -0.4, -0.2)          # outcome model parameters
a <- 3                                          # exposure value of interest
EYa.true <- 0.4 * inv.logit(-2 + 0.7 * a) +     # true dose response curve at a
  0.4 * inv.logit(-2.6 + 0.3 * a) +
  0.1 * inv.logit(-1.6 + 0.5 * a) +
  0.1 * inv.logit(-2.2 + 0.1 * a)

# number of sims per setting
n.rep <- nrow(sim.out) / n_distinct(dplyr::select(sim.out, n, B, vare))

# make long data frame
sim.out.long <- sim.out %>% 
  pivot_longer(cols = starts_with("est") | starts_with("ste"), 
               names_to = "method.param",
               values_to = "val") %>% 
    mutate(method = factor(substr(method.param, 5, 6),
                           levels = c("OL", "OG",
                                      "NL", "NG",
                                      "CL", "CG")),
           name = factor(substr(method.param, 1, 3)),
           EYa.true = EYa.true) %>% 
  dplyr::select(-method.param) %>% 
  group_by(clust, n, B, vare, method, name) %>% 
  mutate(id = row_number()) %>% 
  pivot_wider(names_from = name,
              values_from = val,
              id_cols = c(clust, n, B, vare, method, EYa.true, id))

```

```{r}

# summarize proportion of missing data by setting
sim.out.long %>% 
  group_by(method, n, B, vare) %>% 
  summarise(prop.error = mean(is.na(est))) %>% 
  filter(prop.error > 0) %>% 
  ungroup()

```

Below are boxplots of the empirical distributions of estimators for $\textrm{E}\{Y(3)\}$.

```{r}

# extract simulation parameters
n <- unique(sim.out$n)
B <- unique(sim.out$B)
vare <- unique(sim.out$vare)

# make labels for plots
method.labs <- c("Oracle GLM",
                 "Naive GLM",
                 "Corrected GLM",
                 "Oracle G-Formula",
                 "Naive G-Formula",
                 "Corrected G-Formula")
names(method.labs) <- c("OL", "NL", "CL",
                        "OG", "NG", "CG")

n.labs <- paste0("n = ", n)
names(n.labs) <- n

B.labs <- paste0("B = ", B)
names(B.labs) <- B

vare.labs <- paste0("sigma_e = ", vare)
names(vare.labs) <- vare

# colorblind friendly pallette
pal_light <- c('#EE6677', '#228833', '#4477AA', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB')
pal_dark <- c('#991122', '#114419', '#223b55', '#6b611d', '#117799', '#55193b', '#5d5d5d')

```

```{r}

# separate plots for each sample size
plot.by.var <- function(vare. = 0.25) {
  
  ggplot(
    data = filter(sim.out.long,
                  vare == vare.),
    aes(x = method,
        y = est,
        fill = method,
        color = method)) +
    geom_boxplot() +
    stat_summary(fun = mean,
                 geom = "point",
                 shape = 8,
                 size = 2,
                 orientation = "x",
                 show.legend = F) +
    geom_hline(aes(yintercept = EYa.true),
               linetype = "dashed",
               color = "orange") +
    #facet_grid(~n,
    #           scales = "free",
    #           labeller = labeller(n = n.labs)) +
    labs(y = "Parameter Estimate",
         fill = "Method",
         color = "Method") +
    ggtitle(paste0("Empirical Distribution of Parameter Estimates"),
            subtitle = paste0(n.rep, " simulations per setting")) +
    theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.text.x = element_blank()) +
    scale_fill_manual(values = pal_light,
                      labels = method.labs) +
    scale_color_manual(values = pal_dark,
                      labels = method.labs)
}

```

```{r}

plot.by.var()
plot.by.var() + ylim(-0.8, 1.1)

```

Below is a table of the empirical bias, empirical standard error, and mean estimated standard error of $\widehat{\textrm{E}}\{Y(3)\}$.

```{r}

tbl <- sim.out.long %>% 
  group_by(method, n, B, vare) %>% 
  summarise(bias = mean(est - EYa.true, na.rm = T),
            emp.se = sd(est, na.rm = T),
            est.se = mean(ste)) %>% 
  gather(key, value, bias:est.se) %>% 
  unite(Group, key) %>% 
  spread(Group, value)

setNames(tbl, sub(".+_", "", names(tbl))) %>% 
  kable(digits = 3) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 4,
                     "E{Y(3)}" = 3))
  
```




